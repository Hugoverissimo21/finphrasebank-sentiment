{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4178bda2",
   "metadata": {},
   "source": [
    "# Base Model\n",
    "\n",
    "- Train: 75Agree_train\n",
    "- Test: 75Agree_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def pandas_df_to_Dataset(df, tokenizer):\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset = dataset.map(tokenizer, batched=True)\n",
    "    dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df4e6af",
   "metadata": {},
   "source": [
    "### Step 1: Prepate the data to feed the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba19ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training and test datasets\n",
    "df_train = pd.read_csv('data/75Agree_train.csv')\n",
    "df_test = pd.read_csv('data/75Agree_test.csv')\n",
    "\n",
    "# label encoders\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# label encoding\n",
    "df_train[\"label\"] = df_train[\"label\"].map(label2id)\n",
    "df_test[\"label\"] = df_test[\"label\"].map(label2id)\n",
    "\n",
    "# balance the training set: undersample to minority class\n",
    "df_train = df_train.groupby(\"label\").apply(lambda x: x.sample(df_train[\"label\"].value_counts().min())).reset_index(drop=True)\n",
    "df_train.value_counts(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91bf5e9",
   "metadata": {},
   "source": [
    "### Step 2: Models tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea53dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# prepare for cv\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# set random search area\n",
    "def get_hyperparameters():\n",
    "    return {\n",
    "        \"num_train_epochs\": int(np.random.choice([1, 2, 3])), #1,2,3,4,5\n",
    "        \"learning_rate\": float(np.random.choice([1e-5, 1e-4, 1e-3])), #1e-5, 1e-4, 1e-3, 1e-2\n",
    "        \"weight_decay\": float(np.random.choice([0.05, 0.1, 0.5])), #0.0, 0.01, 0.05, 0.1, 0.5\n",
    "    }\n",
    "\n",
    "# get the cv results dataframe or create a new one\n",
    "try:\n",
    "    all_results = pd.read_csv(\"results.csv\")\n",
    "except FileNotFoundError:\n",
    "    all_results = pd.DataFrame(columns=[\"num_train_epochs\", \"learning_rate\", \"weight_decay\", \"eval_loss\"])\n",
    "\n",
    "# start the hyperparameter tuning\n",
    "for _ in range(120):\n",
    "    current_hyperparameters = get_hyperparameters()\n",
    "    current_hyperparameters_eval_loss = 0\n",
    "    current_results = {\"num_train_epochs\": [], \"learning_rate\": [], \"weight_decay\": [], \"eval_loss\": []}\n",
    "\n",
    "    # make sure the hyperparameters are not already evaluated\n",
    "    mask = (\n",
    "        (all_results['num_train_epochs'] == current_hyperparameters['num_train_epochs']) &\n",
    "        (all_results['learning_rate'] == current_hyperparameters['learning_rate']) &\n",
    "        (all_results['weight_decay'] == current_hyperparameters['weight_decay'])\n",
    "    )\n",
    "    if mask.any():\n",
    "        print(\"Hyperparameters already evaluated, skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Evaluating hyperparameters: {current_hyperparameters}\")\n",
    "    \n",
    "    # tuning hyperparameters\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df_train, df_train[\"label\"])):\n",
    "\n",
    "        # get the folds\n",
    "        df_fold_train = df_train.iloc[train_idx]\n",
    "        df_fold_val = df_train.iloc[val_idx]\n",
    "\n",
    "        # dataset preparation for huggingface transformers\n",
    "        train_dataset = pandas_df_to_Dataset(df_fold_train, tokenize_function)\n",
    "        val_dataset = pandas_df_to_Dataset(df_fold_val, tokenize_function)\n",
    "\n",
    "        train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "        val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "        # the model\n",
    "        model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3, id2label=id2label, label2id=label2id)\n",
    "\n",
    "        # hyperparameters\n",
    "        training_args = TrainingArguments(\n",
    "            num_train_epochs=current_hyperparameters[\"num_train_epochs\"],\n",
    "            learning_rate=current_hyperparameters[\"learning_rate\"],\n",
    "            weight_decay=current_hyperparameters[\"weight_decay\"],\n",
    "            eval_strategy=\"no\",\n",
    "            logging_strategy=\"no\",\n",
    "            report_to=None,\n",
    "            dataloader_pin_memory=False,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            save_total_limit=1,\n",
    "            save_steps=500,\n",
    "            output_dir=f\"./results/fold_{fold}\",\n",
    "            disable_tqdm=True,\n",
    "        )\n",
    "\n",
    "        # training the model\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        trainer.train()\n",
    "\n",
    "        # update the evaluation loss\n",
    "        current_hyperparameters_eval_loss += trainer.evaluate(eval_dataset=val_dataset)[\"eval_loss\"]\n",
    "\n",
    "    # update the results dataframe\n",
    "    current_results[\"num_train_epochs\"].append(current_hyperparameters[\"num_train_epochs\"])\n",
    "    current_results[\"learning_rate\"].append(current_hyperparameters[\"learning_rate\"])\n",
    "    current_results[\"weight_decay\"].append(current_hyperparameters[\"weight_decay\"])\n",
    "    current_results[\"eval_loss\"].append(current_hyperparameters_eval_loss / skf.n_splits)\n",
    "    all_results = pd.concat([all_results, pd.DataFrame(current_results)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a57d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = all_results.sort_values(by=\"eval_loss\").reset_index(drop=True)\n",
    "all_results.to_csv(\"results.csv\", index=False)\n",
    "\n",
    "all_results.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a63b7ec",
   "metadata": {},
   "source": [
    "### Step 3: Extract the best hyperparameters and retrain the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = all_results.iloc[0]\n",
    "print(f\"Best hyperparameters found in {all_results.shape[0]} tested combinations:\")\n",
    "print(f\"Num train epochs: {best_hyperparameters['num_train_epochs']}\")\n",
    "print(f\"Learning rate: {best_hyperparameters['learning_rate']}\")\n",
    "print(f\"Weight decay: {best_hyperparameters['weight_decay']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1101a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# dataset preparation for huggingface transformers\n",
    "train_dataset = pandas_df_to_Dataset(df_train, tokenize_function)\n",
    "test_dataset = pandas_df_to_Dataset(df_test, tokenize_function)\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# the model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3, id2label=id2label, label2id=label2id)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=best_hyperparameters['num_train_epochs'],\n",
    "    learning_rate=best_hyperparameters['learning_rate'],\n",
    "    weight_decay=best_hyperparameters['weight_decay'],\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=25,\n",
    "    report_to=None,\n",
    "    dataloader_pin_memory=False,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_total_limit=1,\n",
    "    save_steps=500,\n",
    "    output_dir=\"./results\",\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965aa880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training and evaluation loss history\n",
    "log_history = trainer.state.log_history\n",
    "train_loss = [x[\"loss\"] for x in log_history if \"loss\" in x]\n",
    "eval_loss = [x[\"eval_loss\"] for x in log_history if \"eval_loss\" in x]\n",
    "eval_steps = [x[\"step\"] for x in log_history if \"eval_loss\" in x]\n",
    "train_steps = [x[\"step\"] for x in log_history if \"loss\" in x]\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(train_steps, train_loss, label=\"Train Loss\")\n",
    "plt.plot(eval_steps, eval_loss, label=\"Eval Loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Evaluation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608ba4e2",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(dataset, split_name):\n",
    "    output = trainer.predict(dataset)\n",
    "    preds = np.argmax(output.predictions, axis=1)\n",
    "    y_true = output.label_ids\n",
    "\n",
    "    print(f\"\\n=== Classification Report ({split_name}) ===\")\n",
    "    print(classification_report(y_true, preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "    cm = confusion_matrix(y_true, preds, normalize='true')\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "                xticklabels=[\"negative\", \"neutral\", \"positive\"],\n",
    "                yticklabels=[\"negative\", \"neutral\", \"positive\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Normalized Confusion Matrix ({split_name})\")\n",
    "    plt.show()\n",
    "\n",
    "# evaluate the model on the training and test datasets\n",
    "evaluate_dataset(train_dataset, \"Train\")\n",
    "evaluate_dataset(test_dataset, \"Test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAA02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
