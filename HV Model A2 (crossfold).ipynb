{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4178bda2",
   "metadata": {},
   "source": [
    "# Base Model\n",
    "\n",
    "- Train: 75Agree_train\n",
    "- Test: 75Agree_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d3f86be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CAA02/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def pandas_df_to_Dataset(df, tokenizer):\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset = dataset.map(tokenizer, batched=True)\n",
    "    dataset = dataset.rename_column(\"label\", \"labels\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df4e6af",
   "metadata": {},
   "source": [
    "### Step 1: Prepate the data to feed the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba19ce88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sz/96b_h5gn3y33k0c7jsvrstmm0000gn/T/ipykernel_29467/918004758.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_train = df_train.groupby(\"label\").apply(lambda x: x.sample(df_train[\"label\"].value_counts().min())).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    336\n",
       "1    336\n",
       "2    336\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the training and test datasets\n",
    "df_train = pd.read_csv('data/75Agree_train.csv')\n",
    "df_test = pd.read_csv('data/75Agree_test.csv')\n",
    "\n",
    "# label encoders\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# label encoding\n",
    "df_train[\"label\"] = df_train[\"label\"].map(label2id)\n",
    "df_test[\"label\"] = df_test[\"label\"].map(label2id)\n",
    "\n",
    "# balance the training set: undersample to minority class\n",
    "df_train = df_train.groupby(\"label\").apply(lambda x: x.sample(df_train[\"label\"].value_counts().min())).reset_index(drop=True)\n",
    "df_train.value_counts(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91bf5e9",
   "metadata": {},
   "source": [
    "### Step 2: Models tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea53dbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating hyperparameters: {'num_train_epochs': 3, 'learning_rate': 0.01, 'weight_decay': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 806/806 [00:00<00:00, 2880.81 examples/s]\n",
      "Map: 100%|██████████| 202/202 [00:00<00:00, 2794.00 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/sz/96b_h5gn3y33k0c7jsvrstmm0000gn/T/ipykernel_29467/1469461548.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 181.4215, 'train_samples_per_second': 13.328, 'train_steps_per_second': 0.843, 'train_loss': 1.6740886214511845, 'epoch': 3.0}\n",
      "{'eval_loss': 1.1167324781417847, 'eval_runtime': 3.897, 'eval_samples_per_second': 51.835, 'eval_steps_per_second': 3.336, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 806/806 [00:00<00:00, 2328.98 examples/s]\n",
      "Map: 100%|██████████| 202/202 [00:00<00:00, 2422.18 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/sz/96b_h5gn3y33k0c7jsvrstmm0000gn/T/ipykernel_29467/1469461548.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 190.4712, 'train_samples_per_second': 12.695, 'train_steps_per_second': 0.803, 'train_loss': 1.5209170073465583, 'epoch': 3.0}\n",
      "{'eval_loss': 1.1091772317886353, 'eval_runtime': 4.1677, 'eval_samples_per_second': 48.469, 'eval_steps_per_second': 3.119, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 806/806 [00:00<00:00, 1902.52 examples/s]\n",
      "Map: 100%|██████████| 202/202 [00:00<00:00, 2120.58 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/sz/96b_h5gn3y33k0c7jsvrstmm0000gn/T/ipykernel_29467/1469461548.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 205.5304, 'train_samples_per_second': 11.765, 'train_steps_per_second': 0.744, 'train_loss': 1.6770897160947713, 'epoch': 3.0}\n",
      "{'eval_loss': 1.1733763217926025, 'eval_runtime': 4.0946, 'eval_samples_per_second': 49.334, 'eval_steps_per_second': 3.175, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 807/807 [00:00<00:00, 2798.78 examples/s]\n",
      "Map: 100%|██████████| 201/201 [00:00<00:00, 2719.81 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/sz/96b_h5gn3y33k0c7jsvrstmm0000gn/T/ipykernel_29467/1469461548.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 211.0826, 'train_samples_per_second': 11.469, 'train_steps_per_second': 0.725, 'train_loss': 1.5228304395488663, 'epoch': 3.0}\n",
      "{'eval_loss': 1.1046470403671265, 'eval_runtime': 4.3414, 'eval_samples_per_second': 46.299, 'eval_steps_per_second': 2.994, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 807/807 [00:00<00:00, 2635.26 examples/s]\n",
      "Map: 100%|██████████| 201/201 [00:00<00:00, 2890.63 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/sz/96b_h5gn3y33k0c7jsvrstmm0000gn/T/ipykernel_29467/1469461548.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 209.29, 'train_samples_per_second': 11.568, 'train_steps_per_second': 0.731, 'train_loss': 1.4886943343418095, 'epoch': 3.0}\n",
      "{'eval_loss': 1.1115427017211914, 'eval_runtime': 4.742, 'eval_samples_per_second': 42.387, 'eval_steps_per_second': 2.741, 'epoch': 3.0}\n",
      "Evaluating hyperparameters: {'num_train_epochs': 4, 'learning_rate': 0.01, 'weight_decay': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 806/806 [00:00<00:00, 2962.79 examples/s]\n",
      "Map: 100%|██████████| 202/202 [00:00<00:00, 2696.93 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/sz/96b_h5gn3y33k0c7jsvrstmm0000gn/T/ipykernel_29467/1469461548.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 293.2736, 'train_samples_per_second': 10.993, 'train_steps_per_second': 0.696, 'train_loss': 1.5009478400735294, 'epoch': 4.0}\n",
      "{'eval_loss': 1.1128426790237427, 'eval_runtime': 4.1314, 'eval_samples_per_second': 48.894, 'eval_steps_per_second': 3.147, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 806/806 [00:00<00:00, 2761.37 examples/s]\n",
      "Map: 100%|██████████| 202/202 [00:00<00:00, 2755.61 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/sz/96b_h5gn3y33k0c7jsvrstmm0000gn/T/ipykernel_29467/1469461548.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 275.2576, 'train_samples_per_second': 11.713, 'train_steps_per_second': 0.741, 'train_loss': 1.5904029397403492, 'epoch': 4.0}\n",
      "{'eval_loss': 1.1196560859680176, 'eval_runtime': 4.1525, 'eval_samples_per_second': 48.645, 'eval_steps_per_second': 3.131, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 806/806 [00:00<00:00, 2845.15 examples/s]\n",
      "Map: 100%|██████████| 202/202 [00:00<00:00, 2814.56 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/sz/96b_h5gn3y33k0c7jsvrstmm0000gn/T/ipykernel_29467/1469461548.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 289.8811, 'train_samples_per_second': 11.122, 'train_steps_per_second': 0.704, 'train_loss': 1.5199000040690105, 'epoch': 4.0}\n",
      "{'eval_loss': 1.108296275138855, 'eval_runtime': 4.4109, 'eval_samples_per_second': 45.795, 'eval_steps_per_second': 2.947, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 807/807 [00:00<00:00, 3055.63 examples/s]\n",
      "Map: 100%|██████████| 201/201 [00:00<00:00, 2916.35 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/sz/96b_h5gn3y33k0c7jsvrstmm0000gn/T/ipykernel_29467/1469461548.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 316.0694, 'train_samples_per_second': 10.213, 'train_steps_per_second': 0.645, 'train_loss': 1.4710057576497395, 'epoch': 4.0}\n",
      "{'eval_loss': 1.1502398252487183, 'eval_runtime': 4.6007, 'eval_samples_per_second': 43.689, 'eval_steps_per_second': 2.826, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 807/807 [00:00<00:00, 2904.77 examples/s]\n",
      "Map: 100%|██████████| 201/201 [00:00<00:00, 2202.89 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/sz/96b_h5gn3y33k0c7jsvrstmm0000gn/T/ipykernel_29467/1469461548.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 299.7498, 'train_samples_per_second': 10.769, 'train_steps_per_second': 0.681, 'train_loss': 1.437822977701823, 'epoch': 4.0}\n",
      "{'eval_loss': 1.1323347091674805, 'eval_runtime': 4.0687, 'eval_samples_per_second': 49.402, 'eval_steps_per_second': 3.195, 'epoch': 4.0}\n",
      "Hyperparameters already evaluated, skipping...\n",
      "Evaluating hyperparameters: {'num_train_epochs': 4, 'learning_rate': 0.001, 'weight_decay': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 806/806 [00:00<00:00, 3056.63 examples/s]\n",
      "Map: 100%|██████████| 202/202 [00:00<00:00, 2183.69 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/sz/96b_h5gn3y33k0c7jsvrstmm0000gn/T/ipykernel_29467/1469461548.py:76: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 83\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# training the model\u001b[39;00m\n\u001b[1;32m     76\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     77\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     78\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     82\u001b[0m )\n\u001b[0;32m---> 83\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# update the evaluation loss\u001b[39;00m\n\u001b[1;32m     86\u001b[0m current_hyperparameters_eval_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAA02/lib/python3.10/site-packages/transformers/trainer.py:2240\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2238\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2241\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/CAA02/lib/python3.10/site-packages/transformers/trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2555\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[1;32m   2557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2558\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2559\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2561\u001b[0m ):\n\u001b[1;32m   2562\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2563\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# prepare for cv\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# set random search area\n",
    "def get_hyperparameters():\n",
    "    return {\n",
    "        \"num_train_epochs\": int(np.random.choice([2, 3, 4, 5])),\n",
    "        \"learning_rate\": float(np.random.choice([1e-5, 1e-4, 1e-3, 1e-2])),\n",
    "        \"weight_decay\": float(np.random.choice([0.0, 0.01, 0.05, 0.1, 0.5])),\n",
    "    }\n",
    "\n",
    "# get the cv results dataframe or create a new one\n",
    "try:\n",
    "    all_results = pd.read_csv(\"results.csv\")\n",
    "except FileNotFoundError:\n",
    "    all_results = pd.DataFrame(columns=[\"num_train_epochs\", \"learning_rate\", \"weight_decay\", \"eval_loss\"])\n",
    "\n",
    "# start the hyperparameter tuning\n",
    "for _ in range(120):\n",
    "    current_hyperparameters = get_hyperparameters()\n",
    "    current_hyperparameters_eval_loss = 0\n",
    "    current_results = {\"num_train_epochs\": [], \"learning_rate\": [], \"weight_decay\": [], \"eval_loss\": []}\n",
    "\n",
    "    # make sure the hyperparameters are not already evaluated\n",
    "    mask = (\n",
    "        (all_results['num_train_epochs'] == current_hyperparameters['num_train_epochs']) &\n",
    "        (all_results['learning_rate'] == current_hyperparameters['learning_rate']) &\n",
    "        (all_results['weight_decay'] == current_hyperparameters['weight_decay'])\n",
    "    )\n",
    "    if mask.any():\n",
    "        print(\"Hyperparameters already evaluated, skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Evaluating hyperparameters: {current_hyperparameters}\")\n",
    "    \n",
    "    # tuning hyperparameters\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df_train, df_train[\"label\"])):\n",
    "\n",
    "        # get the folds\n",
    "        df_fold_train = df_train.iloc[train_idx]\n",
    "        df_fold_val = df_train.iloc[val_idx]\n",
    "\n",
    "        # dataset preparation for huggingface transformers\n",
    "        train_dataset = pandas_df_to_Dataset(df_fold_train, tokenize_function)\n",
    "        val_dataset = pandas_df_to_Dataset(df_fold_val, tokenize_function)\n",
    "\n",
    "        train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "        val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "        # the model\n",
    "        model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3, id2label=id2label, label2id=label2id)\n",
    "\n",
    "        # hyperparameters\n",
    "        training_args = TrainingArguments(\n",
    "            num_train_epochs=current_hyperparameters[\"num_train_epochs\"],\n",
    "            learning_rate=current_hyperparameters[\"learning_rate\"],\n",
    "            weight_decay=current_hyperparameters[\"weight_decay\"],\n",
    "            eval_strategy=\"no\",\n",
    "            logging_strategy=\"no\",\n",
    "            report_to=None,\n",
    "            dataloader_pin_memory=False,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            save_total_limit=1,\n",
    "            save_steps=500,\n",
    "            output_dir=f\"./results/fold_{fold}\",\n",
    "            disable_tqdm=True,\n",
    "        )\n",
    "\n",
    "        # training the model\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        trainer.train()\n",
    "\n",
    "        # update the evaluation loss\n",
    "        current_hyperparameters_eval_loss += trainer.evaluate(eval_dataset=val_dataset)[\"eval_loss\"]\n",
    "\n",
    "    # update the results dataframe\n",
    "    current_results[\"num_train_epochs\"].append(current_hyperparameters[\"num_train_epochs\"])\n",
    "    current_results[\"learning_rate\"].append(current_hyperparameters[\"learning_rate\"])\n",
    "    current_results[\"weight_decay\"].append(current_hyperparameters[\"weight_decay\"])\n",
    "    current_results[\"eval_loss\"].append(current_hyperparameters_eval_loss / skf.n_splits)\n",
    "    all_results = pd.concat([all_results, pd.DataFrame(current_results)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a80a57d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = all_results.sort_values(by=\"eval_loss\").reset_index(drop=True)\n",
    "all_results.to_csv(\"results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c052e300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_train_epochs</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>eval_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.347579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.348092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.374654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.474633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.790321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_train_epochs  learning_rate  weight_decay  eval_loss\n",
       "0                 2        0.00010          0.10   0.347579\n",
       "1                 4        0.00010          0.10   0.348092\n",
       "2                 3        0.00010          0.00   0.374654\n",
       "3                 5        0.00010          0.01   0.474633\n",
       "4                 2        0.00001          0.05   0.790321"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a63b7ec",
   "metadata": {},
   "source": [
    "### Step 3: Extract the best hyperparameters and retrain the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608ba4e2",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a77fb7f",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# TO BE DONE YET FOR THE CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039695bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train_losses, eval_losses = [], []\n",
    "train_steps, eval_steps = [], []\n",
    "for fold in logs:\n",
    "    log_history = logs[fold]\n",
    "    train_losses.append([x[\"loss\"] for x in log_history if \"loss\" in x])\n",
    "    eval_losses.append([x[\"eval_loss\"] for x in log_history if \"eval_loss\" in x])\n",
    "    train_steps.append([x[\"step\"] for x in log_history if \"loss\" in x])\n",
    "    eval_steps.append([x[\"step\"] for x in log_history if \"eval_loss\" in x])\n",
    "\n",
    "# get mean and std for train and eval losses\n",
    "train_losses = np.array(train_losses)\n",
    "train_loss = np.mean(train_losses, axis=0)\n",
    "train_loss_std = np.std(train_losses, axis=0)\n",
    "\n",
    "eval_losses = np.array(eval_losses)\n",
    "eval_loss = np.mean(eval_losses, axis=0)\n",
    "eval_loss_std = np.std(eval_losses, axis=0)\n",
    "    \n",
    "# make sure train_steps and eval_steps sublists are equal\n",
    "if all(steps == train_steps[0] for steps in train_steps):\n",
    "    train_steps = train_steps[0]\n",
    "else:\n",
    "    raise ValueError(\"Train steps are not consistent across folds.\")\n",
    "\n",
    "if all(steps == eval_steps[0] for steps in eval_steps):\n",
    "    eval_steps = eval_steps[0]\n",
    "else:\n",
    "    raise ValueError(\"Eval steps are not consistent across folds.\")\n",
    "\n",
    "# draw the training and evaluation losses\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(train_steps, train_loss, label=\"Train Loss\")\n",
    "plt.plot(eval_steps, eval_loss, label=\"Eval Loss\")\n",
    "plt.fill_between(train_steps, train_loss - train_loss_std, train_loss + train_loss_std, alpha=0.2, color='blue')\n",
    "plt.fill_between(eval_steps, eval_loss - eval_loss_std, eval_loss + eval_loss_std, alpha=0.2, color='orange')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Evaluation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6696d518",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"train_dataset = Dataset.from_pandas(df_fold_train)\n",
    "val_dataset = Dataset.from_pandas(df_fold_val)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "val_dataset = val_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_dataset(dataset, split_name):\n",
    "    output = trainer.predict(dataset)\n",
    "    preds = np.argmax(output.predictions, axis=1)\n",
    "    y_true = output.label_ids\n",
    "\n",
    "    print(f\"\\n=== Classification Report ({split_name}) ===\")\n",
    "    print(classification_report(y_true, preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "    cm = confusion_matrix(y_true, preds, normalize='true')\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "                xticklabels=[\"negative\", \"neutral\", \"positive\"],\n",
    "                yticklabels=[\"negative\", \"neutral\", \"positive\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Normalized Confusion Matrix ({split_name})\")\n",
    "    plt.show()\n",
    "\n",
    "# evaluate the model on the training and test datasets\n",
    "evaluate_dataset(train_dataset, \"Train\")\n",
    "evaluate_dataset(test_dataset, \"Test\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182e9e43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAA02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
