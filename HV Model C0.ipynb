{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6e71cb",
   "metadata": {},
   "source": [
    "# Weighted Dataset Model\n",
    "\n",
    "- Train: AnyAgree_train\n",
    "- Test: 75Agree_test\n",
    "\n",
    "# TODO\n",
    "\n",
    "- everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac5ab70",
   "metadata": {},
   "source": [
    "https://chatgpt.com/c/683f09e6-2b90-800f-b478-2cc480d87d50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07262df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_scheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Define dataset\n",
    "class WeightedTextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, weight_classes, tokenizer, max_len, weight_map):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_len)\n",
    "        self.labels = labels\n",
    "        self.weights = [weight_map[w] for w in weight_classes]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        item['weights'] = torch.tensor(self.weights[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Step 2: Prepare tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Optional GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Step 3: Dummy data\n",
    "texts = [\"bad service\", \"great experience\", \"not good\", \"excellent\", \"horrible wait\"]\n",
    "labels = [0, 1, 0, 1, 0]                          # true labels\n",
    "ref_classes = [1, 2, 1, 2, 1]                     # weight-basis classes\n",
    "weight_map = {1: 2.0, 2: 1.0}                     # ref_class => weight\n",
    "\n",
    "# Step 4: Dataloaders\n",
    "train_dataset = WeightedTextDataset(texts, labels, ref_classes, tokenizer, max_len=32, weight_map=weight_map)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Step 5: Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer,\n",
    "                             num_warmup_steps=0,\n",
    "                             num_training_steps=len(train_loader)*3)\n",
    "\n",
    "# Step 6: Training loop\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                        attention_mask=batch[\"attention_mask\"],\n",
    "                        labels=batch[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        with torch.no_grad():\n",
    "            per_sample_loss = torch.nn.functional.cross_entropy(\n",
    "                logits, batch[\"labels\"], reduction='none')\n",
    "            weighted_loss = (per_sample_loss * batch[\"weights\"]).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        weighted_loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "# Step 7: Evaluation\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                        attention_mask=batch[\"attention_mask\"])\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(batch[\"labels\"].cpu().tolist())\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(\"Accuracy:\", acc)\n",
    "\"\"\"\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e34858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAA02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
