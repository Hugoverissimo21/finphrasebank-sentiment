{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6e71cb",
   "metadata": {},
   "source": [
    "# Weighted Dataset Model\n",
    "\n",
    "- Train: AnyAgree_train\n",
    "- Test: 75Agree_test\n",
    "\n",
    "# TODO\n",
    "\n",
    "- everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac5ab70",
   "metadata": {},
   "source": [
    "https://chatgpt.com/c/683f09e6-2b90-800f-b478-2cc480d87d50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f973d00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CAA02/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import get_scheduler\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6440ed3d",
   "metadata": {},
   "source": [
    "### Step 1: Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd2f14fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agreements in training set: ['66Agree' '50Agree' '75Agree' 'AllAgree']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>agreement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>66Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>50Agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>positive</td>\n",
       "      <td>75Agree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence     label agreement\n",
       "0  Technopolis plans to develop in stages an area...   neutral   66Agree\n",
       "1  The international electronic industry company ...  negative   50Agree\n",
       "2  With the new production plant the company woul...  positive   75Agree"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the training and test datasets\n",
    "df_train = pd.read_csv('data/AnyAgree_train.csv')\n",
    "df_test = pd.read_csv('data/75Agree_test.csv')\n",
    "\n",
    "\n",
    "print(f\"Agreements in training set: {df_train['agreement'].unique()}\")\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31121528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sz/96b_h5gn3y33k0c7jsvrstmm0000gn/T/ipykernel_34760/2547942190.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_train = df_train.groupby(\"label\").apply(lambda x: x.sample(df_train[\"label\"].value_counts().min())).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    520\n",
       "1    520\n",
       "2    520\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label encoders\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# label encoding\n",
    "df_train[\"label\"] = df_train[\"label\"].map(label2id)\n",
    "df_test[\"label\"] = df_test[\"label\"].map(label2id)\n",
    "\n",
    "# balance the training set: undersample to minority class\n",
    "df_train = df_train.groupby(\"label\").apply(lambda x: x.sample(df_train[\"label\"].value_counts().min())).reset_index(drop=True)\n",
    "df_train.value_counts(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107b0e09",
   "metadata": {},
   "source": [
    "### Step 2: Create the weighted pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30292bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the datasets class (with or without weights)\n",
    "class WeightedTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, weight_map):\n",
    "        texts = df[\"sentence\"].tolist()\n",
    "        labels = df[\"label\"].tolist()\n",
    "        weight_classes = df[\"agreement\"].tolist()\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=128)\n",
    "        self.labels = labels\n",
    "        self.weights = [weight_map[w] for w in weight_classes]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        item['weights'] = torch.tensor(self.weights[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "class WeightlessTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        texts = df[\"sentence\"].tolist()\n",
    "        labels = df[\"label\"].tolist()\n",
    "\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=128)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# define the tokenizer    \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# define the weight map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4183c90c",
   "metadata": {},
   "source": [
    "**Weight Map**\n",
    "\n",
    "The ideia is that the higher the agreement level, the higher the weight, so the model will focus more on the samples with higher agreement.\n",
    "\n",
    "But the weights should also take into account the number of samples in each agreement level so the cost function can be compared to a weightless dataset.\n",
    "\n",
    "With that in mind, the weight map should be dynamically calculated based on the number of samples in each agreement level following this formula:\n",
    "\n",
    "$\\ $\n",
    "\n",
    "Let $a_1, a_2, \\ldots, a_k$ be the raw agreement scores for each class (e.g., 0.5, 0.66, 0.75, 1.0)\n",
    "\n",
    "Let $ n_i $ be the number of samples in class $ i $, and $ N = \\sum_{i=1}^k n_i $\n",
    "\n",
    "Let $ p_i $ be the softmax weight for class $ i $, and $ w_i $ be the final weight for class $ i $\n",
    "\n",
    "$$\n",
    "w_i = \\text{scale} \\cdot p_i = \\left( \\frac{N}{\\sum_{j=1}^k n_j \\cdot p_j} \\right) \\cdot  \\left( \\frac{e^{a_i}}{\\sum_{j=1}^k e^{a_j}} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50319cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weight_map(df):\n",
    "    \"\"\"\n",
    "    Compute a weight map using softmax-normalized agreement scores, scaled so the mean weight is 1.\n",
    "    \"\"\"\n",
    "    a_i = {\n",
    "        \"50Agree\": 0.50,\n",
    "        \"66Agree\": 0.66,\n",
    "        \"75Agree\": 0.75,\n",
    "        \"AllAgree\": 1.00\n",
    "    }\n",
    "    n_i = {agreement: df['agreement'].value_counts().get(agreement, 0) for agreement in a_i.keys()}\n",
    "    N = len(df)\n",
    "\n",
    "    # compute softmax weights (p_i)\n",
    "    p_i = {agreement: np.exp(a_i[agreement]) for agreement in a_i}\n",
    "    p_i = {agreement: value/ sum(p_i.values()) for agreement, value in p_i.items()}\n",
    "\n",
    "    # compute scale (scale)\n",
    "    scale = N / sum(n_i[agreement] * p_i[agreement] for agreement in p_i)\n",
    "\n",
    "    # compute weight map\n",
    "    weight_map = {agreement: scale * p_i[agreement] for agreement in p_i}\n",
    "\n",
    "    # return the weight map\n",
    "    return weight_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46a00a",
   "metadata": {},
   "source": [
    "### Step 3: Tune the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5440ad67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating hyperparameters: {'num_train_epochs': 3, 'learning_rate': 1e-05, 'weight_decay': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 78/78 [02:52<00:00,  2.22s/it]\n",
      "100%|██████████| 78/78 [02:52<00:00,  2.22s/it]\n",
      "100%|██████████| 78/78 [03:05<00:00,  2.38s/it]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 78/78 [02:59<00:00,  2.31s/it]\n",
      "100%|██████████| 78/78 [03:02<00:00,  2.34s/it]\n",
      " 28%|██▊       | 22/78 [00:53<02:21,  2.52s/it]"
     ]
    }
   ],
   "source": [
    "# prepare for cv\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# set random search area\n",
    "def get_hyperparameters():\n",
    "    return {\n",
    "        \"num_train_epochs\": int(np.random.choice([1, 2, 3, 4, 5])),\n",
    "        \"learning_rate\": float(np.random.choice([1e-5, 1e-4, 1e-3, 1e-2])),\n",
    "        \"weight_decay\": float(np.random.choice([0.0, 0.01, 0.05, 0.1, 0.5])),\n",
    "    }\n",
    "\n",
    "try:\n",
    "    all_results = pd.read_csv(\"HV Model C.csv\")\n",
    "except FileNotFoundError:\n",
    "    all_results = pd.DataFrame(columns=[\"num_train_epochs\", \"learning_rate\", \"weight_decay\", \"eval_loss\"])\n",
    "\n",
    "# start the hyperparameter tuning\n",
    "for _ in range(2): #range(25)\n",
    "    current_hyperparameters = get_hyperparameters()\n",
    "    current_hyperparameters_eval_loss = 0\n",
    "    current_results = {\"num_train_epochs\": [], \"learning_rate\": [], \"weight_decay\": [], \"eval_loss\": []}\n",
    "\n",
    "    # make sure the hyperparameters are not already evaluated\n",
    "    mask = (\n",
    "        (all_results['num_train_epochs'] == current_hyperparameters['num_train_epochs']) &\n",
    "        (all_results['learning_rate'] == current_hyperparameters['learning_rate']) &\n",
    "        (all_results['weight_decay'] == current_hyperparameters['weight_decay'])\n",
    "    )\n",
    "    if mask.any():\n",
    "        print(\"Hyperparameters already evaluated, skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Evaluating hyperparameters: {current_hyperparameters}\")\n",
    "    \n",
    "    # tuning hyperparameters\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df_train, df_train[\"label\"])):\n",
    "\n",
    "        # get the folds\n",
    "        df_fold_train = df_train.iloc[train_idx]\n",
    "        df_fold_val = df_train.iloc[val_idx]\n",
    "\n",
    "        # dataset preparation for the model\n",
    "        train_dataset = WeightedTextDataset(df_fold_train, tokenizer, get_weight_map(df_fold_train))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "        val_dataset = WeightedTextDataset(df_fold_val, tokenizer, get_weight_map(df_fold_val))\n",
    "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "        # define the model\n",
    "        device = torch.device(\"cpu\")\n",
    "        model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3, id2label=id2label, label2id=label2id)\n",
    "        model.to(device)\n",
    "\n",
    "        # setup the otimizer and scheduler\n",
    "        optimizer = AdamW(model.parameters(),\n",
    "                        lr=current_hyperparameters[\"learning_rate\"],\n",
    "                        weight_decay=current_hyperparameters[\"weight_decay\"])\n",
    "\n",
    "        lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer,\n",
    "                                    num_warmup_steps=0,\n",
    "                                    num_training_steps=len(train_loader) * current_hyperparameters[\"num_train_epochs\"])\n",
    "            \n",
    "\n",
    "        # training the model\n",
    "        for epoch in range(current_hyperparameters[\"num_train_epochs\"]):\n",
    "            model.train()\n",
    "            for batch in tqdm(train_loader):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                                attention_mask=batch[\"attention_mask\"],\n",
    "                                labels=batch[\"labels\"])\n",
    "                logits = outputs.logits\n",
    "\n",
    "                per_sample_loss = torch.nn.functional.cross_entropy(\n",
    "                    logits, batch[\"labels\"], reduction='none')\n",
    "                weighted_loss = (per_sample_loss * batch[\"weights\"]).mean()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                weighted_loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "\n",
    "        # weighted validation loss\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                                attention_mask=batch[\"attention_mask\"])\n",
    "                logits = outputs.logits\n",
    "                loss = torch.nn.functional.cross_entropy(logits, batch[\"labels\"], reduction='none')\n",
    "                weighted_loss = (loss * batch[\"weights\"]).mean()\n",
    "                val_losses.append(weighted_loss.item())\n",
    "        val_mean = sum(val_losses) / len(val_losses)\n",
    "        print(f\"Fold {fold + 1}/{skf.n_splits}, Validation Loss: {val_mean:.4f}\")\n",
    "        \n",
    "        # update the evaluation loss\n",
    "        current_hyperparameters_eval_loss += val_mean\n",
    "\n",
    "    # update the results dataframe\n",
    "    current_results[\"num_train_epochs\"].append(current_hyperparameters[\"num_train_epochs\"])\n",
    "    current_results[\"learning_rate\"].append(current_hyperparameters[\"learning_rate\"])\n",
    "    current_results[\"weight_decay\"].append(current_hyperparameters[\"weight_decay\"])\n",
    "    current_results[\"eval_loss\"].append(current_hyperparameters_eval_loss / skf.n_splits)\n",
    "    all_results = pd.concat([all_results, pd.DataFrame(current_results)], ignore_index=True)\n",
    "    all_results.to_csv(\"HV Model C.csv\", index=False)\n",
    "\n",
    "\"Hyperparameter tuning completed. Results saved to HV Model C.csv.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33f0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = all_results.sort_values(by=\"eval_loss\").reset_index(drop=True)\n",
    "all_results.to_csv(\"HV Model C.csv\", index=False)\n",
    "\n",
    "all_results.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726dd38e",
   "metadata": {},
   "source": [
    "### Step 4: Train the model with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9881008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = all_results.iloc[0]\n",
    "print(f\"Best hyperparameters found in {all_results.shape[0]} tested combinations:\")\n",
    "print(f\"Num train epochs: {best_hyperparameters['num_train_epochs']}\")\n",
    "print(f\"Learning rate: {best_hyperparameters['learning_rate']}\")\n",
    "print(f\"Weight decay: {best_hyperparameters['weight_decay']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0ca3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the datasets\n",
    "train_dataset = WeightedTextDataset(df_train, tokenizer, get_weight_map(df_train))\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_dataset = WeightlessTextDataset(df_test, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "device = torch.device(\"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3, id2label=id2label, label2id=label2id)\n",
    "model.to(device)\n",
    "\n",
    "# setup the otimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=best_hyperparameters[\"learning_rate\"],\n",
    "                  weight_decay=best_hyperparameters[\"weight_decay\"])\n",
    "\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer,\n",
    "                             num_warmup_steps=0,\n",
    "                             num_training_steps=len(train_loader) * best_hyperparameters[\"num_train_epochs\"])\n",
    "\n",
    "# training the model with loss tracking\n",
    "train_loss_per_epoch = []\n",
    "val_loss_per_epoch = []\n",
    "\n",
    "for epoch in range(best_hyperparameters[\"num_train_epochs\"]):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                        attention_mask=batch[\"attention_mask\"],\n",
    "                        labels=batch[\"labels\"])\n",
    "        logits = outputs.logits\n",
    "\n",
    "        per_sample_loss = torch.nn.functional.cross_entropy(\n",
    "            logits, batch[\"labels\"], reduction='none')\n",
    "        weighted_loss = (per_sample_loss * batch[\"weights\"]).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        weighted_loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        epoch_losses.append(weighted_loss.item())\n",
    "\n",
    "    mean_train_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    train_loss_per_epoch.append(mean_train_loss)\n",
    "    print(f\"Epoch {epoch+1}, Train Weighted Loss: {mean_train_loss:.4f}\")\n",
    "\n",
    "    # weightless validation loss for the test set\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                            attention_mask=batch[\"attention_mask\"])\n",
    "            logits = outputs.logits\n",
    "            loss = torch.nn.functional.cross_entropy(logits, batch[\"labels\"], reduction='mean')\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    val_mean = sum(val_losses) / len(val_losses)\n",
    "    val_loss_per_epoch.append(val_mean)\n",
    "    print(f\"Epoch {epoch+1}, Val Unweighted Loss: {val_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77624792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the learning curves\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(train_loss_per_epoch, label='Train Weighted Loss')\n",
    "plt.plot(val_loss_per_epoch, label='Val Unweighted Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Evaluation Loss:: VARIA COM SEED\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db3d472",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed53a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(dataset, split_name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(input_ids=batch[\"input_ids\"],\n",
    "                            attention_mask=batch[\"attention_mask\"])\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "    print(f\"\\n=== Classification Report ({split_name}) ===\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds, normalize='true')\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "                xticklabels=[\"negative\", \"neutral\", \"positive\"],\n",
    "                yticklabels=[\"negative\", \"neutral\", \"positive\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Normalized Confusion Matrix ({split_name})\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "semi_train_df = pd.read_csv('data/75Agree_train.csv')\n",
    "semi_train_df[\"label\"] = semi_train_df[\"label\"].map(label2id)\n",
    "semi_train_dataset = WeightlessTextDataset(semi_train_df, tokenizer)\n",
    "\n",
    "df_test = pd.read_csv('data/75Agree_test.csv')\n",
    "df_test[\"label\"] = df_test[\"label\"].map(label2id)\n",
    "test_dataset = WeightlessTextDataset(df_test, tokenizer)\n",
    "\n",
    "# evaluate the model on the training and test datasets\n",
    "evaluate_dataset(semi_train_dataset, \"Partial Train (weightless 75% Agreement)\")\n",
    "evaluate_dataset(test_dataset, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebfb3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAA02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
