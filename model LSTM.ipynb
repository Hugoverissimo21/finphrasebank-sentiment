{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4178bda2",
   "metadata": {},
   "source": [
    "# Base Model - using LSTM\n",
    "\n",
    "- Train: 75Agree_train\n",
    "- Test: 75Agree_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df4e6af",
   "metadata": {},
   "source": [
    "### Step 1: Prepate the data to feed the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba19ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training and test datasets\n",
    "df_train = pd.read_csv('data/75Agree_train.csv')\n",
    "df_test = pd.read_csv('data/75Agree_test.csv')\n",
    "\n",
    "# label encoders\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# label encoding\n",
    "df_train[\"label\"] = df_train[\"label\"].map(label2id)\n",
    "df_test[\"label\"] = df_test[\"label\"].map(label2id)\n",
    "\n",
    "# balance the training set: undersample to minority class\n",
    "df_train = df_train.groupby(\"label\").apply(lambda x: x.sample(df_train[\"label\"].value_counts().min())).reset_index(drop=True)\n",
    "df_train.value_counts(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c64a2ea",
   "metadata": {},
   "source": [
    "### Step 2: Train the model with fine-tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c329ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df_train['sentence'])\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(df_train['sentence'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(df_test['sentence'])\n",
    "\n",
    "# padding\n",
    "lengths = df_train['sentence'].str.split().apply(len)\n",
    "maxlen = int(np.percentile(lengths, 95))\n",
    "\n",
    "X_train = pad_sequences(X_train_seq, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test_seq, maxlen=maxlen)\n",
    "\n",
    "# labels\n",
    "y_train = df_train['label'].values\n",
    "y_test = df_test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d95193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters():\n",
    "    return {\n",
    "        \"embedding_dim\": random.choice([64, 128, 256]),\n",
    "        \"lstm_units\": random.choice([64, 128, 256]),\n",
    "        \"epochs\": random.choice([5, 8, 10, 15]),\n",
    "        \"lr\": random.choice([1e-3, 5e-4, 1e-4]),\n",
    "        \"dropout\": random.choice([0.2, 0.3, 0.5]),\n",
    "        \"recurrent_dropout\": random.choice([0.0, 0.2, 0.3])\n",
    "    }\n",
    "\n",
    "# cross-validation setup\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "try:\n",
    "    all_results = pd.read_csv(\"model LSTM.csv\")\n",
    "except FileNotFoundError:\n",
    "    all_results = pd.DataFrame(columns=[\"embedding_dim\", \"lstm_units\", \"epochs\", \"lr\", \"dropout\", \"recurrent_dropout\", \"valloss\"])\n",
    "\n",
    "\n",
    "for _ in range(2):\n",
    "    current_hyperparams = get_hyperparameters()\n",
    "    current_results = {\"embedding_dim\": [], \"lstm_units\": [], \"epochs\": [], \"lr\": [], \"dropout\": [], \"recurrent_dropout\": [], \"valloss\": []}\n",
    "    current_valloss = 0\n",
    "\n",
    "    mask = (\n",
    "        (all_results['embedding_dim'] == current_hyperparams['embedding_dim']) &\n",
    "        (all_results['lstm_units'] == current_hyperparams['lstm_units']) &\n",
    "        (all_results['epochs'] == current_hyperparams['epochs']) &\n",
    "        (all_results['lr'] == current_hyperparams['lr']) &\n",
    "        (all_results['dropout'] == current_hyperparams['dropout']) &\n",
    "        (all_results['recurrent_dropout'] == current_hyperparams['recurrent_dropout'])\n",
    "    )\n",
    "\n",
    "    if mask.any():\n",
    "        print(\"Hyperparameters already evaluated, skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Evaluating hyperparameters: {current_hyperparams}\")\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X_train, y_train)):\n",
    "        print(f\"Fold {fold + 1}\", end=' | ')\n",
    "\n",
    "        # split data\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        # model definition\n",
    "        model = Sequential([\n",
    "            Embedding(input_dim=10000, output_dim=current_hyperparams['embedding_dim'], input_length=maxlen),\n",
    "            LSTM(current_hyperparams['lstm_units'],\n",
    "                 dropout=current_hyperparams['dropout'],\n",
    "                 recurrent_dropout=current_hyperparams['recurrent_dropout']),\n",
    "            Dense(3, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "        # train\n",
    "        history = model.fit(X_train_fold, y_train_fold, epochs=current_hyperparams['epochs'], batch_size=32, validation_data=(X_val_fold, y_val_fold), verbose=0)\n",
    "\n",
    "        # evaluate\n",
    "        val_loss = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
    "        print(f\"Validation Loss: {val_loss}\")\n",
    "\n",
    "        current_valloss += val_loss\n",
    "\n",
    "    current_valloss /= skf.n_splits\n",
    "    current_results['embedding_dim'].append(current_hyperparams['embedding_dim'])\n",
    "    current_results['lstm_units'].append(current_hyperparams['lstm_units'])\n",
    "    current_results['epochs'].append(current_hyperparams['epochs'])\n",
    "    current_results['lr'].append(current_hyperparams['lr'])\n",
    "    current_results['dropout'].append(current_hyperparams['dropout'])\n",
    "    current_results['recurrent_dropout'].append(current_hyperparams['recurrent_dropout'])\n",
    "    current_results['valloss'].append(current_valloss)\n",
    "    all_results = pd.concat([all_results, pd.DataFrame(current_results)], ignore_index=True)\n",
    "    all_results.to_csv(\"model LSTM.csv\", index=False)\n",
    "\n",
    "\"Hyperparameter tuning completed. Results saved to model LSTM.csv.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6881271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = all_results.sort_values(by=\"eval_loss\").reset_index(drop=True)\n",
    "all_results.to_csv(\"model LSTM.csv\", index=False)\n",
    "\n",
    "all_results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac3fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams = all_results.iloc[0]\n",
    "\n",
    "# final model with the best hyperparameters\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=best_hyperparams['embedding_dim'], input_length=maxlen),\n",
    "    LSTM(best_hyperparams['lstm_units'],\n",
    "         dropout=best_hyperparams['dropout'],\n",
    "         recurrent_dropout=best_hyperparams['recurrent_dropout']),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# train\n",
    "history = model.fit(X_train, y_train, epochs=best_hyperparams['epochs'], batch_size=32, validation_data=(X_test, y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4b0ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curves\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Test')\n",
    "plt.title('Learning Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91bf5e9",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e102acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação com seaborn\n",
    "def print_metrics(X, y, title):\n",
    "    y_pred_probs = model.predict(X)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    print(f\"=== {title} ===\")\n",
    "    print(classification_report(y, y_pred, target_names=[id2label[i] for i in range(3)], digits=3))\n",
    "\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    labels = [id2label[i] for i in range(3)]\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap='Blues')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.ylabel('Verdadeiro')\n",
    "    plt.title(f'Matriz de Confusão - {title}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print_metrics(X_train, y_train, \"Train\")\n",
    "print_metrics(X_test, y_test, \"Test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAA02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
