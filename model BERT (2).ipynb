{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e6e71cb",
   "metadata": {},
   "source": [
    "# Data Augmentation Model\n",
    "\n",
    "- Train: 75Agree_train\n",
    "- Test: 75Agree_test\n",
    "\n",
    "# TODO\n",
    "\n",
    "- missing cv for hyp tunning\n",
    "\n",
    "- organize better the code\n",
    "\n",
    "- data augmentation hyperparameters (tune?) and get better examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4353f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CAA02/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to /Users/hugover/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from nltk.corpus import wordnet\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "#spacy.cli.download(\"en_core_web_sm\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"figure.figsize\": (5, 5*0.8),\n",
    "    \"savefig.dpi\": 300,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ef70c1",
   "metadata": {},
   "source": [
    "### Step 1: Read the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d5ffd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sz/96b_h5gn3y33k0c7jsvrstmm0000gn/T/ipykernel_61448/918004758.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_train = df_train.groupby(\"label\").apply(lambda x: x.sample(df_train[\"label\"].value_counts().min())).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    336\n",
       "1    336\n",
       "2    336\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the training and test datasets\n",
    "df_train = pd.read_csv('data/75Agree_train.csv')\n",
    "df_test = pd.read_csv('data/75Agree_test.csv')\n",
    "\n",
    "# label encoders\n",
    "label2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# label encoding\n",
    "df_train[\"label\"] = df_train[\"label\"].map(label2id)\n",
    "df_test[\"label\"] = df_test[\"label\"].map(label2id)\n",
    "\n",
    "# balance the training set: undersample to minority class\n",
    "df_train = df_train.groupby(\"label\").apply(lambda x: x.sample(df_train[\"label\"].value_counts().min())).reset_index(drop=True)\n",
    "df_train.value_counts(\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffbb1e9",
   "metadata": {},
   "source": [
    "### Step 2: Create the data augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01014a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128,\n",
    "                 p_back=0.3, p_syn=0.4, p_ner=0.2, p_none=0.3):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.p_back = p_back\n",
    "        self.p_syn = p_syn\n",
    "        self.p_ner = p_ner\n",
    "        self.p_none = p_none\n",
    "\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "        en_de_model_name = \"Helsinki-NLP/opus-mt-en-de\"\n",
    "        de_en_model_name = \"Helsinki-NLP/opus-mt-de-en\"\n",
    "        self.en_de_tok = MarianTokenizer.from_pretrained(en_de_model_name)\n",
    "        self.de_en_tok = MarianTokenizer.from_pretrained(de_en_model_name)\n",
    "        self.en_de_mod = MarianMTModel.from_pretrained(en_de_model_name)\n",
    "        self.de_en_mod = MarianMTModel.from_pretrained(de_en_model_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.loc[idx, 'sentence']\n",
    "        label = self.df.loc[idx, 'label']\n",
    "\n",
    "        # Randomly apply augmentation techniques based on probabilities\n",
    "        if not random.random() < self.p_none:\n",
    "            if random.random() < self.p_back:\n",
    "                text = self.back_translate(text) or text\n",
    "            if random.random() < self.p_syn:\n",
    "                text = self.synonym_replace(text) or text\n",
    "            if random.random() < self.p_ner:\n",
    "                text = self.ner_replace(text) or text\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze() for k, v in encoding.items()}\n",
    "        item[\"labels\"] = torch.tensor(label)\n",
    "        return item\n",
    "\n",
    "    def back_translate(self, text):\n",
    "        try:\n",
    "            de = self.translate(text, self.en_de_tok, self.en_de_mod)\n",
    "            return self.translate(de, self.de_en_tok, self.de_en_mod)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def translate(self, text, tokenizer, model):\n",
    "        batch = tokenizer([text], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        gen = model.generate(**batch)\n",
    "        return tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "\n",
    "    def get_synonyms(self, word):\n",
    "        syns = set()\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for l in syn.lemmas():\n",
    "                w = l.name().replace('_', ' ')\n",
    "                if w.lower() != word.lower():\n",
    "                    syns.add(w)\n",
    "        return list(syns)\n",
    "\n",
    "    def synonym_replace(self, text, p=0.2):\n",
    "        words = text.split()\n",
    "        out = []\n",
    "        for word in words:\n",
    "            if random.random() < p:\n",
    "                syns = self.get_synonyms(word)\n",
    "                if syns:\n",
    "                    out.append(random.choice(syns))\n",
    "                    continue\n",
    "            out.append(word)\n",
    "        return ' '.join(out)\n",
    "\n",
    "    def ner_replace(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        template, pool = self.extract_template(doc)\n",
    "        if not pool:\n",
    "            return None\n",
    "        return self.fill_template(template, pool)\n",
    "\n",
    "    def extract_template(self, doc):\n",
    "        entity_pool = {}\n",
    "        template = doc.text\n",
    "        offsets = []\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in [\"ORG\", \"DATE\", \"EVENT\"]:\n",
    "                tag = f\"<{ent.label_}>\"\n",
    "                offsets.append((ent.start_char, ent.end_char, tag, ent.text, ent.label_))\n",
    "                if ent.label_ not in entity_pool:\n",
    "                    entity_pool[ent.label_] = set()\n",
    "                entity_pool[ent.label_].add(ent.text)\n",
    "\n",
    "        offsets.sort()\n",
    "        new_text = \"\"\n",
    "        last = 0\n",
    "        for start, end, tag, _, _ in offsets:\n",
    "            new_text += template[last:start] + tag\n",
    "            last = end\n",
    "        new_text += template[last:]\n",
    "        return new_text, entity_pool\n",
    "\n",
    "    def fill_template(self, template, entity_pool):\n",
    "        out = template\n",
    "        for label, values in entity_pool.items():\n",
    "            tag = f\"<{label}>\"\n",
    "            if tag in out and values:\n",
    "                out = out.replace(tag, random.choice(list(values)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "243a1636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The operating margin of Aker Yards Cruise & Ferries division went down from 8.3 % to 6.4 % in the first quarter of 2007 .\n",
      "the operating security deposit of aker yards cruise & ferries division went down from 8. 3 % to 6. 4 % in the first tail of 2007.\n",
      "\n",
      "The total capital of funds managed by the bank decreased by 28 % to EUR 284mn by the end of September 2008 .\n",
      "the total capital of funds managed by the bank decreased by 28 % to eur 284mn by the end of september 2008.\n",
      "\n",
      "In Finland , snow storms brought trees down on power lines , cutting off electricity for some 2,000 households .\n",
      "in finland, snowstorms brought down trees on power lines, which thinned off some 2, 000 households'electricity.\n",
      "\n",
      "EMSA Deputy Chairman of the Board Juri Lember told BNS on Wednesday that this was the first time he heard about the strike as the Swedish side had not informed the Estonian union yet .\n",
      "emsa deputy chairman of the board juri lember told bns on wednesday that this was the first time he heard about the strike as the swedish side had not informed the estonian union yet.\n",
      "\n",
      "Repeats sees 2008 operating profit down y-y ( Reporting by Helsinki Newsroom ) Keywords : TECNOMEN-RESULTS\n",
      "repeats sees 2008 operating profit down y - y ( reporting by helsinki newsroom ) keywords : tecnomen - results\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# apply the augmentation\n",
    "dataset = AugmentedTextDataset(df_train, tokenizer, p_none=0)\n",
    "\n",
    "# get some examples of the augmentation\n",
    "for i in range(5):\n",
    "    original = df_train.loc[i, 'sentence']\n",
    "    augmented = dataset[i]\n",
    "    decoded = tokenizer.decode(augmented['input_ids'], skip_special_tokens=True)\n",
    "    print(f\"{original}\\n{decoded}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d84e8",
   "metadata": {},
   "source": [
    "### Step 3: Cross validation for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d53781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hyperparameter tuning completed. Results saved to model BERT (2).csv.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# prepare for cv\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# set random search area\n",
    "def get_hyperparameters():\n",
    "    return {\n",
    "        \"num_train_epochs\": int(np.random.choice([1, 2, 3, 4, 5])), #\n",
    "        \"learning_rate\": float(np.random.choice([1e-5, 1e-4, 1e-3, 1e-2])), #\n",
    "        \"weight_decay\": float(np.random.choice([0.0, 0.01, 0.05, 0.1, 0.5])), #\n",
    "    }\n",
    "\n",
    "# get the cv results dataframe or create a new one\n",
    "try:\n",
    "    all_results = pd.read_csv(\"model BERT (2).csv\")\n",
    "except FileNotFoundError:\n",
    "    all_results = pd.DataFrame(columns=[\"num_train_epochs\", \"learning_rate\", \"weight_decay\", \"eval_loss\"])\n",
    "\n",
    "# start the hyperparameter tuning\n",
    "for _ in range(0): #range(25)\n",
    "    current_hyperparameters = get_hyperparameters()\n",
    "    current_hyperparameters_eval_loss = 0\n",
    "    current_results = {\"num_train_epochs\": [], \"learning_rate\": [], \"weight_decay\": [], \"eval_loss\": []}\n",
    "\n",
    "    # make sure the hyperparameters are not already evaluated\n",
    "    mask = (\n",
    "        (all_results['num_train_epochs'] == current_hyperparameters['num_train_epochs']) &\n",
    "        (all_results['learning_rate'] == current_hyperparameters['learning_rate']) &\n",
    "        (all_results['weight_decay'] == current_hyperparameters['weight_decay'])\n",
    "    )\n",
    "    if mask.any():\n",
    "        print(\"Hyperparameters already evaluated, skipping...\")\n",
    "        continue\n",
    "    else:\n",
    "        print(f\"Evaluating hyperparameters: {current_hyperparameters}\")\n",
    "    \n",
    "    # tuning hyperparameters\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(df_train, df_train[\"label\"])):\n",
    "\n",
    "        # get the folds\n",
    "        df_fold_train = df_train.iloc[train_idx]\n",
    "        df_fold_val = df_train.iloc[val_idx]\n",
    "\n",
    "        # dataset preparation for huggingface transformers\n",
    "        train_dataset = AugmentedTextDataset(df_fold_train, tokenizer)\n",
    "        val_dataset = AugmentedTextDataset(df_fold_val, tokenizer, p_none=1)\n",
    "\n",
    "        # the model\n",
    "        model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3, id2label=id2label, label2id=label2id)\n",
    "\n",
    "        # hyperparameters\n",
    "        training_args = TrainingArguments(\n",
    "            num_train_epochs=current_hyperparameters[\"num_train_epochs\"],\n",
    "            learning_rate=current_hyperparameters[\"learning_rate\"],\n",
    "            weight_decay=current_hyperparameters[\"weight_decay\"],\n",
    "            eval_strategy=\"no\",\n",
    "            logging_strategy=\"no\",\n",
    "            report_to=None,\n",
    "            dataloader_pin_memory=False,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            save_total_limit=1,\n",
    "            save_steps=500,\n",
    "            output_dir=f\"./results/fold_{fold}\",\n",
    "            disable_tqdm=True,\n",
    "        )\n",
    "\n",
    "        # training the model\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        trainer.train()\n",
    "\n",
    "        # update the evaluation loss\n",
    "        current_hyperparameters_eval_loss += trainer.evaluate(eval_dataset=val_dataset)[\"eval_loss\"]\n",
    "\n",
    "    # update the results dataframe\n",
    "    current_results[\"num_train_epochs\"].append(current_hyperparameters[\"num_train_epochs\"])\n",
    "    current_results[\"learning_rate\"].append(current_hyperparameters[\"learning_rate\"])\n",
    "    current_results[\"weight_decay\"].append(current_hyperparameters[\"weight_decay\"])\n",
    "    current_results[\"eval_loss\"].append(current_hyperparameters_eval_loss / skf.n_splits)\n",
    "    all_results = pd.concat([all_results, pd.DataFrame(current_results)], ignore_index=True)\n",
    "    all_results.to_csv(\"model BERT (2).csv\", index=False)\n",
    "\n",
    "\"Hyperparameter tuning completed. Results saved to model BERT (2).csv.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b398d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_train_epochs</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>eval_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.303190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.306246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.400804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_train_epochs  learning_rate  weight_decay  eval_loss\n",
       "0                 2        0.00010          0.10   0.303190\n",
       "1                 3        0.00010          0.10   0.306246\n",
       "2                 5        0.00001          0.01   0.400804"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = all_results.sort_values(by=\"eval_loss\").reset_index(drop=True)\n",
    "all_results.to_csv(\"model BERT (2).csv\", index=False)\n",
    "\n",
    "all_results.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189527e7",
   "metadata": {},
   "source": [
    "### Step 4: Create the model with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27966a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found in 15 tested combinations:\n",
      "Num train epochs: 2.0\n",
      "Learning rate: 0.0001\n",
      "Weight decay: 0.1\n"
     ]
    }
   ],
   "source": [
    "best_hyperparameters = all_results.iloc[0]\n",
    "print(f\"Best hyperparameters found in {all_results.shape[0]} tested combinations:\")\n",
    "print(f\"Num train epochs: {best_hyperparameters['num_train_epochs']}\")\n",
    "print(f\"Learning rate: {best_hyperparameters['learning_rate']}\")\n",
    "print(f\"Weight decay: {best_hyperparameters['weight_decay']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42676724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/sz/96b_h5gn3y33k0c7jsvrstmm0000gn/T/ipykernel_61448/2065444062.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "# dataset preparation for huggingface transformers\n",
    "train_dataset = AugmentedTextDataset(df_train, tokenizer)\n",
    "test_dataset = AugmentedTextDataset(df_test, tokenizer, p_none=1)\n",
    "\n",
    "# the model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3, id2label=id2label, label2id=label2id)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    num_train_epochs=best_hyperparameters['num_train_epochs'],\n",
    "    learning_rate=best_hyperparameters['learning_rate'],\n",
    "    weight_decay=best_hyperparameters['weight_decay'],\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=25,\n",
    "    report_to=None,\n",
    "    dataloader_pin_memory=False,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    save_total_limit=1,\n",
    "    save_steps=500,\n",
    "    output_dir=\"./results\",\n",
    ")\n",
    "\n",
    "# train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff78c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training and evaluation loss history\n",
    "log_history = trainer.state.log_history\n",
    "train_loss = [x[\"loss\"] for x in log_history if \"loss\" in x]\n",
    "eval_loss = [x[\"eval_loss\"] for x in log_history if \"eval_loss\" in x]\n",
    "eval_steps = [x[\"step\"] for x in log_history if \"eval_loss\" in x]\n",
    "train_steps = [x[\"step\"] for x in log_history if \"loss\" in x]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_steps, train_loss, label=\"Training Loss\")\n",
    "plt.plot(eval_steps, eval_loss, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning Curve for Data Augmented BERT Model\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"assets/data_augmented_bert_learninc_curve.png\", transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0690c592",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca5ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AugmentedTextDataset(df_train, tokenizer, p_none=1)\n",
    "test_dataset = AugmentedTextDataset(df_test, tokenizer, p_none=1)\n",
    "\n",
    "def evaluate_dataset(dataset, split_name):\n",
    "    output = trainer.predict(dataset)\n",
    "    preds = np.argmax(output.predictions, axis=1)\n",
    "    y_true = output.label_ids\n",
    "\n",
    "    print(f\"\\n=== Classification Report ({split_name}) ===\")\n",
    "    print(classification_report(y_true, preds, target_names=[\"negative\", \"neutral\", \"positive\"]))\n",
    "\n",
    "    cm = confusion_matrix(y_true, preds, normalize='true')\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "                xticklabels=[\"negative\", \"neutral\", \"positive\"],\n",
    "                yticklabels=[\"negative\", \"neutral\", \"positive\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix ({split_name})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"assets/dataaugmented_bert_confusion_matrix_{split_name}.png\", transparent=True)\n",
    "    plt.show()\n",
    "\n",
    "# evaluate the model on the training and test datasets\n",
    "evaluate_dataset(train_dataset, \"Train\")\n",
    "evaluate_dataset(test_dataset, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7eef6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAA02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
